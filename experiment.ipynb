{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a series of examples demonstrating the use of the icanswim/qchem repo \n",
    "## for quantum mechanic geometric machine learning utilizing pytorch, pyg and rdkit.\n",
    "## This is a demonstration of the use of the icanswim/cosmosis repo for \n",
    "## data science and machine learning projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys # required for relative imports in jupyter lab\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning,\n",
    "                            message='TypedStorage is deprecated')\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from cosmosis.model import FFNet\n",
    "from cosmosis.dataset import SKDS, Pad1d, Flatten, Reshape, SqueezeN, Index, EmbedLookup\n",
    "\n",
    "from learning import Learn, Selector\n",
    "from dataset import QM7, QM7b, QM7X, ANI1x, QM9, PGDS\n",
    "from model import GraphNet, PygModel, EncoderLoss, GraphNetVariationalEncoder \n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss, L1Loss, NLLLoss, CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading QM9 datadic from a pickled copy...\n",
      "CDataset created...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_input': {'X': array([ 6.       ,  1.       ,  1.       ,  1.       ,  1.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       , 36.858112 ,\n",
       "         10.765888 , 10.765641 , 10.765863 , 10.765677 ,  0.9157932,\n",
       "          0.5      ,  1.7942736,  1.7943105,  1.7942796,  0.9157932,\n",
       "          1.7943147,  0.5      ,  1.7943105,  1.7942796,  0.9157932,\n",
       "          1.7943147,  1.7942736,  0.5      ,  1.7942796,  0.9157932,\n",
       "          1.7943147,  1.7942736,  1.7943105,  0.5      ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  4.       ,\n",
       "          1.       ,  1.       ,  1.       ,  1.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "          0.       ,  0.       ,  0.       ], dtype=float32),\n",
       "  'distance': array([0.       , 1.091953 , 1.0919516, 1.0919464, 1.0919476, 1.091953 ,\n",
       "         0.       , 1.7831198, 1.7831475, 1.7831566, 1.0919516, 1.7831198,\n",
       "         0.       , 1.7831576, 1.7831483, 1.0919464, 1.7831475, 1.7831576,\n",
       "         0.       , 1.7831478, 1.0919476, 1.7831566, 1.7831483, 1.7831478,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "         0.       ], dtype=float32),\n",
       "  'embed': [[array([5, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0])],\n",
       "   [array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0])]],\n",
       "  'adjacency': array([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0], dtype=int32),\n",
       "  'mulliken': array([-0.535689,  0.133921,  0.133922,  0.133923,  0.133923,  0.      ,\n",
       "          0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "          0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "          0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "          0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
       "        dtype=float32)},\n",
       " 'criterion_input': {'target': array([-40.47893], dtype=float32)}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example exploring cosmosis/qchem QM9 dataset \n",
    "#use the qchem rdkit branch for rdkit data augmentation\n",
    "\n",
    "qm9_features = ['A','B','C','mu','alpha','homo','lumo', 'gap','r2','zpve',\n",
    "                'U0','U','H','G','Cv','qm9_n_atoms','qm9_block','qm9_atom_type',\n",
    "                'qm9_xyz','mulliken','in_file','smile','distance','coulomb']\n",
    "\n",
    "rdkit_features = ['atom_type','atomic_number','aromatic','chirality',\n",
    "                  'degree','charge','n_hs','n_rads','hybridization',\n",
    "                  'edge_indices','edge_attr','rdmol_block','n_atoms',\n",
    "                  'xyz','distance','coulomb','adjacency','rdmol']\n",
    "\n",
    "rdkit_edge = ['edge_indices','edge_attr']\n",
    "\n",
    "lookup_hybrid = QM9.embed_lookup['hybridization']\n",
    "lookup_chiral = QM9.embed_lookup['chirality']\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': [\n",
    "                                                                   'atomic_number',\n",
    "                                                                   'coulomb',\n",
    "                                                                   'aromatic',\n",
    "                                                                   'degree',\n",
    "                                                                  ],\n",
    "                                                             #'edge_idx': ['edge_indices'],\n",
    "                                                             #'edge_attr': ['edge_attr'],\n",
    "                                                             #'coulomb': ['coulomb'],\n",
    "                                                             'distance': ['distance'],\n",
    "                                                             'embed': ['hybridization','chirality'],\n",
    "                                                             'adjacency': ['adjacency'],\n",
    "                                                             'mulliken': ['mulliken'],\n",
    "                                                            },\n",
    "                                             'criterion_input': {'target': ['U0']}},\n",
    "                              'transforms': {\n",
    "                                             'hybridization': [Pad1d(29), EmbedLookup(lookup_hybrid)],\n",
    "                                             'chirality': [Pad1d(29), EmbedLookup(lookup_chiral)],\n",
    "                                             'atomic_number': [Pad1d(29)],\n",
    "                                             'aromatic': [Pad1d(29)],\n",
    "                                             'degree': [Pad1d(29)],\n",
    "                                             'mulliken': [Pad1d(29)],\n",
    "                                             'coulomb': [Reshape((-1)), Pad1d(29*29)],\n",
    "                                             'distance': [Reshape((-1)), Pad1d(29*29)],\n",
    "                                             'adjacency': [Reshape((-1)), Pad1d(29*29)],\n",
    "                                            },\n",
    "                              #'filter_on': ('n_atoms','<','10'), \n",
    "                              'n': 100, #non-random subset for testing,\n",
    "                              'use_pickle': 'qm9_full_ds_0_conf', #use pickle if exists otherwise create\n",
    "                              'n_conformers': 0}}\n",
    "\n",
    "qm9 = QM9(**ds_params['train_params'])\n",
    "qm9[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading QM9 datadic from a pickled copy...\n",
      "CDataset created...\n",
      "FFNet model loaded...\n",
      "CModel loaded...\n",
      "running model on gpu...\n",
      "learning time: 0:00:20.805855\n",
      "epoch: 0, lr: 0.01\n",
      "train loss: 23.76692381772128, val loss: 9.932946666393404\n",
      "learning time: 0:00:39.991860\n",
      "epoch: 1, lr: 0.01\n",
      "train loss: 9.772065172662268, val loss: 8.441601996328316\n",
      "learning time: 0:00:57.963720\n",
      "epoch: 2, lr: 0.01\n",
      "train loss: 8.751800214327298, val loss: 7.65050440520243\n",
      "learning time: 0:01:15.888599\n",
      "epoch: 3, lr: 0.01\n",
      "train loss: 8.037050490612751, val loss: 7.365472528669569\n",
      "learning time: 0:01:33.916941\n",
      "epoch: 4, lr: 0.01\n",
      "train loss: 7.594898419947057, val loss: 7.742629088607489\n",
      "learning time: 0:01:51.830921\n",
      "epoch: 5, lr: 0.01\n",
      "train loss: 7.279943350145033, val loss: 6.389289650262571\n",
      "learning time: 0:02:10.002826\n",
      "epoch: 6, lr: 0.01\n",
      "train loss: 7.185043511023888, val loss: 7.932905028848087\n",
      "learning time: 0:02:28.005023\n",
      "epoch: 7, lr: 0.01\n",
      "train loss: 6.72577021472104, val loss: 6.491113266913719\n",
      "learning time: 0:02:45.931662\n",
      "epoch: 8, lr: 0.005\n",
      "train loss: 6.556045232786166, val loss: 7.299182757832646\n",
      "learning time: 0:03:03.993232\n",
      "epoch: 9, lr: 0.005\n",
      "train loss: 5.77534016229056, val loss: 5.43171582190819\n",
      "learning time: 0:03:22.013022\n",
      "epoch: 10, lr: 0.005\n",
      "train loss: 5.5956766461992595, val loss: 5.149468063529021\n",
      "learning time: 0:03:40.150473\n",
      "epoch: 11, lr: 0.005\n",
      "train loss: 5.549336218333744, val loss: 5.08902494580138\n",
      "learning time: 0:03:58.239134\n",
      "epoch: 12, lr: 0.005\n",
      "train loss: 5.528876449011422, val loss: 5.372359752655029\n",
      "learning time: 0:04:16.431619\n",
      "epoch: 13, lr: 0.005\n",
      "train loss: 5.341229468792468, val loss: 4.987325020085752\n",
      "learning time: 0:04:34.607199\n",
      "epoch: 14, lr: 0.005\n",
      "train loss: 5.323847091781509, val loss: 5.507369078841864\n",
      "learning time: 0:04:52.785484\n",
      "epoch: 15, lr: 0.005\n",
      "train loss: 5.26746770251881, val loss: 5.000719257429535\n",
      "learning time: 0:05:10.916479\n",
      "epoch: 16, lr: 0.005\n",
      "train loss: 5.256591923253519, val loss: 4.992273011238747\n",
      "learning time: 0:05:30.134838\n",
      "epoch: 17, lr: 0.005\n",
      "train loss: 5.2617733725301035, val loss: 4.983932446810155\n",
      "learning time: 0:05:50.287785\n",
      "epoch: 18, lr: 0.005\n",
      "train loss: 5.2613264634059025, val loss: 5.003263249116785\n",
      "learning time: 0:06:10.333158\n",
      "epoch: 19, lr: 0.005\n",
      "train loss: 5.121591147842941, val loss: 5.526447489370708\n",
      "learning time: 0:06:30.523726\n",
      "epoch: 20, lr: 0.005\n",
      "train loss: 5.168649256979669, val loss: 4.888590119243447\n",
      "learning time: 0:06:51.223418\n",
      "epoch: 21, lr: 0.005\n",
      "train loss: 5.042264984370945, val loss: 4.810813900692011\n",
      "learning time: 0:07:11.475872\n",
      "epoch: 22, lr: 0.005\n",
      "train loss: 5.039256971532648, val loss: 5.771952077454212\n",
      "learning time: 0:07:32.481341\n",
      "epoch: 23, lr: 0.005\n",
      "train loss: 4.940175572975532, val loss: 4.914397736780004\n",
      "learning time: 0:07:53.432118\n",
      "epoch: 24, lr: 0.005\n",
      "train loss: 4.97687098796551, val loss: 4.838359122182808\n",
      "learning time: 0:08:14.187610\n",
      "epoch: 25, lr: 0.005\n",
      "train loss: 4.939659821570336, val loss: 5.174829523547802\n",
      "learning time: 0:08:34.973473\n",
      "epoch: 26, lr: 0.005\n",
      "train loss: 5.031701813210974, val loss: 4.738260875340381\n",
      "learning time: 0:08:55.612982\n",
      "epoch: 27, lr: 0.005\n",
      "train loss: 4.8233654395683665, val loss: 4.621376369513717\n",
      "learning time: 0:09:16.269160\n",
      "epoch: 28, lr: 0.005\n",
      "train loss: 4.950219831600056, val loss: 4.933221053453832\n",
      "learning time: 0:09:36.636731\n",
      "epoch: 29, lr: 0.005\n",
      "train loss: 4.933518898570454, val loss: 5.608246401244519\n",
      "test loss: 5.472611321343316\n",
      "learning time: 0:09:38.595804\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#example cosmosis/qchem QM9 dataset with filter\n",
    "#use the qchem rdkit branch for rdkit data augmentation\n",
    "\n",
    "qm9_features = ['A','B','C','mu','alpha','homo','lumo', 'gap','r2','zpve',\n",
    "                'U0','U','H','G','Cv','qm9_n_atoms','qm9_block','qm9_atom_type',\n",
    "                'qm9_xyz','mulliken','in_file','smile','distance','coulomb']\n",
    "\n",
    "rdkit_features = ['atom_type','atomic_number','aromatic','chirality',\n",
    "                  'degree','charge','n_hs','n_rads','hybridization',\n",
    "                  'edge_indices','edge_attr','rdmol_block','n_atoms',\n",
    "                  'xyz','distance','coulomb','adjacency','rdmol']\n",
    "\n",
    "rdkit_edge = ['edge_indices','edge_attr']\n",
    "\n",
    "lookup_hybrid = QM9.embed_lookup['hybridization']\n",
    "lookup_chiral = QM9.embed_lookup['chirality']\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': [\n",
    "                                                                   'atomic_number',\n",
    "                                                                   'coulomb',\n",
    "                                                                   'aromatic',\n",
    "                                                                   'degree',\n",
    "                                                                  ],\n",
    "                                                             #'edge_idx': ['edge_indices'],\n",
    "                                                             #'edge_attr': ['edge_attr'],\n",
    "                                                             #'coulomb': ['coulomb'],\n",
    "                                                             #'distance': ['distance'],\n",
    "                                                             'embed': ['hybridization','chirality'],\n",
    "                                                             #'adjacency': ['adjacency'],\n",
    "                                                            },\n",
    "                                             'criterion_input': {'target': ['U0']}},\n",
    "                              'transforms': {\n",
    "                                             'hybridization': [Pad1d(29), EmbedLookup(lookup_hybrid)],\n",
    "                                             'chirality': [Pad1d(29), EmbedLookup(lookup_chiral)],\n",
    "                                             'atomic_number': [Pad1d(29)],\n",
    "                                             'aromatic': [Pad1d(29)],\n",
    "                                             'degree': [Pad1d(29)],\n",
    "                                             #'mulliken': [Pad1d(29)],\n",
    "                                             'coulomb': [Reshape((-1)), Pad1d(29*29)],\n",
    "                                            },\n",
    "                              #'filter_on': ('n_atoms','<','10'), \n",
    "                              #'n': 10000, #non-random subset for testing,\n",
    "                              'use_pickle': 'qm9_full_ds_0_conf', #load if exists otherwise create\n",
    "                              'n_conformers': 0}}\n",
    "\n",
    "model_params = {'in_channels': 29+29+29+29*29+29*8+29*8, \n",
    "                'hidden': 4096, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel',\n",
    "                'embed_params': [('hybridization',9,8,0,True),('chirality',4,8,0,True)],\n",
    "               }      \n",
    "\n",
    "metrics_params = {'report_interval': 1,\n",
    "                  'log_plot': False}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15),\n",
    "                 #'subset': .1, #create random subset\n",
    "                }\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "\n",
    "l = Learn([QM9], FFNet, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=L1Loss,\n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          squeeze_y_pred=False, batch_size=128, epochs=30, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example cosmosis/qchem QM9 dataset with filter\n",
    "#use the qchem rdkit branch for rdkit data augmentation\n",
    "\n",
    "qm9_features = ['A','B','C','mu','alpha','homo','lumo', 'gap','r2','zpve',\n",
    "                'U0','U','H','G','Cv','qm9_n_atoms','qm9_block','qm9_atom_type',\n",
    "                'qm9_xyz','mulliken','in_file','smile','distance','coulomb']\n",
    "\n",
    "rdkit_features = ['atom_type','atomic_number','aromatic','chirality',\n",
    "                  'degree','charge','n_hs','n_rads','hybridization',\n",
    "                  'edge_indices','edge_attr','rdmol_block','n_atoms',\n",
    "                  'xyz','distance','coulomb','adjacency','rdmol']\n",
    "\n",
    "rdkit_edge = ['edge_indices','edge_attr']\n",
    "\n",
    "lookup_hybrid = QM9.embed_lookup['hybridization']\n",
    "lookup_chiral = QM9.embed_lookup['chirality']\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': [\n",
    "                                                                   'atomic_number',\n",
    "                                                                   'coulomb',\n",
    "                                                                   'aromatic',\n",
    "                                                                   'degree',\n",
    "                                                                  ],\n",
    "                                                             #'edge_idx': ['edge_indices'],\n",
    "                                                             #'edge_attr': ['edge_attr'],\n",
    "                                                             #'coulomb': ['coulomb'],\n",
    "                                                             #'distance': ['distance'],\n",
    "                                                             'embed': ['hybridization','chirality'],\n",
    "                                                             #'adjacency': ['adjacency'],\n",
    "                                                            },\n",
    "                                             'criterion_input': {'target': ['U0']}},\n",
    "                              'transforms': {\n",
    "                                             'hybridization': [Pad1d(29), EmbedLookup(lookup_hybrid)],\n",
    "                                             'chirality': [Pad1d(29), EmbedLookup(lookup_chiral)],\n",
    "                                             'atomic_number': [Pad1d(29)],\n",
    "                                             'aromatic': [Pad1d(29)],\n",
    "                                             'degree': [Pad1d(29)],\n",
    "                                             #'mulliken': [Pad1d(29)],\n",
    "                                             'coulomb': [Reshape((-1)), Pad1d(29*29)],\n",
    "                                            },\n",
    "                              #'filter_on': ('n_atoms','<','10'), \n",
    "                              'n': 10000, #non-random subset for testing,\n",
    "                              'use_pickle': 'qm9_test_ds', #load if exists otherwise create\n",
    "                              'n_conformers': 10}}\n",
    "\n",
    "model_params = {'in_channels': 29+29+29+29*29+8*29+29*8, \n",
    "                'hidden': 4096, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel',\n",
    "                'embed_params': [('hybridization',9,8,0,True),('chirality',4,8,0,True)],\n",
    "               }      \n",
    "\n",
    "metrics_params = {'report_interval': 1,\n",
    "                  'log_plot': False}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15),\n",
    "                 #'subset': .2, #create random subset\n",
    "                }\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "\n",
    "l = Learn([QM9], FFNet, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=L1Loss,\n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          squeeze_y_pred=False, batch_size=32, epochs=2, save_model=True, \n",
    "          load_model='20240417_0916.pth', load_embed='20240417_0916')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.load('./models/20240417_0916_0_embedding_weight.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example exploring cosmosis/qchem QM9 dataset\n",
    "\n",
    "l, s, li, si = 0, 9999, 0, 0\n",
    "for i in qm9.ds_idx:\n",
    "    d = qm9[i]['model_input']['X'].shape[0]\n",
    "    if d > l:\n",
    "        l = d\n",
    "        li = i\n",
    "    if d < s:\n",
    "        s = d\n",
    "        si = i\n",
    "print('longest molecule index: ', li, ' length: ', l)\n",
    "print('shortest molecule index: ', si, ' length: ', s)\n",
    "qm9[si]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example exploring cosmosis/qchem PyG dataset wrapper with Data object output\n",
    "\n",
    "import copy\n",
    "class IndexY(T.BaseTransform):\n",
    "    def __init__(self, i):\n",
    "        self.i = i\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        # Shallow-copy the data so that we prevent in-place data modification.\n",
    "        return self.forward(copy.copy(data))\n",
    "    \n",
    "    def forward(self, data):\n",
    "        y = data.y[:,self.i]\n",
    "        data.y = y\n",
    "        return data\n",
    "\n",
    "ds_params = {'train_params': {'dataset': 'QM9',\n",
    "                              'pg_params': {'root': './data/',\n",
    "                                            'transform': IndexY(1)}}}\n",
    "\n",
    "pgds = PGDS(**ds_params['train_params'])\n",
    "ds_idx = np.arange(1000)\n",
    "selector = Selector(train_idx=ds_idx)\n",
    "selector('train')\n",
    "loader = DataLoader(pgds, batch_size=2, sampler=selector)\n",
    "batch = next(iter(loader))\n",
    "print('batch: ', batch)\n",
    "print('batch.ptr: ', batch.ptr)\n",
    "print('batch.idx: ', batch.idx)\n",
    "print('batch.x: ', batch.x)\n",
    "print('pgds[1]: ', pgds[1])\n",
    "print('pgds[1].x: ', pgds[1].x)\n",
    "print('pgds[1].y: ', pgds[1].y)\n",
    "print('pgds[1].idx: ', pgds[1].idx)\n",
    "print('pgds[1].edge_index: ', pgds[1].edge_index)\n",
    "print('pgds[1].edge_attr: ', pgds[1].edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example cosmosis/qchem QM9 dataset with filter\n",
    "#use the qchem rdkit branch for rdkit data augmentation\n",
    "\n",
    "qm9_features = ['A','B','C','mu','alpha','homo','lumo', 'gap','r2','zpve',\n",
    "                'U0','U','H','G','Cv','qm9_n_atoms','qm9_block','qm9_atom_type',\n",
    "                'qm9_xyz','mulliken','in_file','smile','distance','coulomb']\n",
    "\n",
    "rdkit_features = ['atom_type','atomic_number','aromatic','chirality',\n",
    "                  'degree','charge','n_hs','n_rads','hybridization',\n",
    "                  'edge_indices','edge_attr','rdmol_block','n_atoms',\n",
    "                  'xyz','distance','coulomb','adjacency','rdmol']\n",
    "\n",
    "rdkit_edge = ['edge_indices','edge_attr']\n",
    "\n",
    "qm9_features = ['A','B','C','mu','alpha','homo','lumo', 'gap','r2','zpve',\n",
    "                'U0','U','H','G','Cv','qm9_n_atoms','qm9_block','qm9_atom_type',\n",
    "                'qm9_xyz','mulliken','in_file','smile','distance','coulomb']\n",
    "\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': ['coulomb']},\n",
    "                                             'criterion_input': {'target': ['U0']}},\n",
    "                              'n': 10000, #non-random subset for testing\n",
    "                              'transforms': {'coulomb': [Pad((29,29)),Reshape((-1))],\n",
    "                                             'U0': [SqueezeN()]},\n",
    "                              'filter_on': ('n_atoms','>','10'), #filter out molecules with less than 10 atoms\n",
    "                              'use_pickle': False}}\n",
    "\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': ['aromatic','degree',\n",
    "                                                                   'charge','coulomb'],\n",
    "                                                             'embed': ['hybridization','chirality']},\n",
    "                                             'criterion_input': {'target': ['U0']}},\n",
    "                              'n': 10000, #non-random subset for testing\n",
    "                              'n_conformers': 10,\n",
    "                              'pad': 29,\n",
    "                              'pad_feats': ['mulliken','coulomb','aromatic','degree',\n",
    "                                            'charge','n_hs','n_rads','atomic_number',\n",
    "                                            'hybridization','chirality'],\n",
    "                              'as_tensor': True,\n",
    "                              'filter_on': ('n_atoms','>','10'), #filter out molecules with less than 10 atoms\n",
    "                              'use_pickle': False,\n",
    "                              'flatten': True}}\n",
    "\n",
    "model_params = {'in_channels': 29*29, \n",
    "                'hidden': 4096, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel'\n",
    "               }\n",
    "\n",
    "model_params = {'in_channels': 29+29+29+29*29+29*16+29*16, \n",
    "                'hidden': 4096, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel',\n",
    "                'embed_params': [('hybridization',9,16,0,True),('chirality',5,16,0,True)]\n",
    "               }      \n",
    "\n",
    "metrics_params = {'report_interval': 1,\n",
    "                  'log_plot': False}\n",
    "\n",
    "metrics_params = {'report_interval': 1,\n",
    "                  'log_plot': False}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15)}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15),\n",
    "                 'subset': .2 #create random subset\n",
    "                 \n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 5,\n",
    "                'cooldown': 2}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "\n",
    "\n",
    "l = Learn([QM9], FFNet, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=L1Loss,\n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          squeeze_y_pred=True, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example cosmosis/qchem QM9 dataset with filter, rdkit data augmentation, \n",
    "#multiple conformations, embeddings and custom model \n",
    "rdkit_features = ['atom_type','atomic_number','aromatic','chirality',\n",
    "                  'degree','charge','n_hs','n_rads','hybridization',\n",
    "                  'edge_indices','edge_attr','rdmol_block','n_atoms',\n",
    "                  'xyz','distance','coulomb','adjacency','rdmol']\n",
    "\n",
    "rdkit_edge = ['edge_indices','edge_attr']\n",
    "\n",
    "qm9_features = ['A','B','C','mu','alpha','homo','lumo', 'gap','r2','zpve',\n",
    "                'U0','U','H','G','Cv','qm9_n_atoms','qm9_block','qm9_atom_type',\n",
    "                'qm9_xyz','mulliken','in_file','smile','distance','coulomb']\n",
    "\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': ['aromatic','degree',\n",
    "                                                                   'charge','coulomb'],\n",
    "                                                             'embed': ['hybridization','chirality']},\n",
    "                                             'criterion_input': {'target': ['U0']}},\n",
    "                              'n': 10000, #non-random subset for testing\n",
    "                              'n_conformers': 10,\n",
    "                              'pad': 29,\n",
    "                              'pad_feats': ['mulliken','coulomb','aromatic','degree',\n",
    "                                            'charge','n_hs','n_rads','atomic_number',\n",
    "                                            'hybridization','chirality'],\n",
    "                              'as_tensor': True,\n",
    "                              'filter_on': ('n_atoms','>','10'), #filter out molecules with less than 10 atoms\n",
    "                              'use_pickle': False,\n",
    "                              'flatten': True}}\n",
    "\n",
    "\n",
    "model_params = {'in_channels': 29+29+29+29*29+29*16+29*16, \n",
    "                'hidden': 4096, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel',\n",
    "                'embed_params': [('hybridization',9,16,0,True),('chirality',5,16,0,True)]\n",
    "               }\n",
    "                \n",
    "metrics_params = {'report_interval': 1,\n",
    "                  'log_plot': False}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15),\n",
    "                 'subset': .2 #create random subset\n",
    "                }\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 5,\n",
    "                'cooldown': 2}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "l = Learn([QM9], FFNet, Selector, Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=L1Loss, \n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example cosmosis/qchem PyG dataset wrapper with Data object output with\n",
    "#custom transforms and sklearn metrics and custom PyG model\n",
    "\n",
    "class IndexY(T.BaseTransform):\n",
    "    def __init__(self, i):\n",
    "        self.i = i\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        #Shallow-copy the data so that we prevent in-place data modification.\n",
    "        return self.forward(copy.copy(data))\n",
    "    \n",
    "    def forward(self, data):\n",
    "        y = data.y[:,self.i]\n",
    "        data.y = y\n",
    "        return data\n",
    "\n",
    "model_params = {'in_channels': 11, \n",
    "                'hidden': 128, \n",
    "                'out_channels': 1,\n",
    "                'depth': 1,\n",
    "                'convolution': 'NetConv',\n",
    "                'pool': 'MeanAggregation',\n",
    "                'dropout': .1,\n",
    "                'softmax': None,\n",
    "                'activation': 'relu',\n",
    "                'edge_features': 4,\n",
    "               }\n",
    "\n",
    "ds_params = {'train_params': {'dataset': 'QM9',\n",
    "                              'transforms': {},\n",
    "                              'pg_params': {'root': './data/',\n",
    "                                            'transform': T.Compose([IndexY(0), \n",
    "                                                                   ])}}}\n",
    "\n",
    "metrics_params = {'report_interval': 10,\n",
    "                  'log_plot': False}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7, .15)}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "\n",
    "l = Learn([PGDS], GraphNet, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=MSELoss, DataLoader=DataLoader,\n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          metrics_params=metrics_params,\n",
    "          squeeze_y_pred=True, batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example graph variational autoencoder without adversarial regulation and GCNConv encoder\n",
    "\n",
    "model_params = {'in_channels': 11, \n",
    "                'hidden': 32, \n",
    "                'out_channels': 32,\n",
    "                'depth': 2,\n",
    "                'softmax': None,\n",
    "                'pool': None,\n",
    "                'convolution': 'GCNConv'}\n",
    "\n",
    "ds_params = {'train_params': {'dataset': 'QM9',\n",
    "                              'transforms': {},\n",
    "                              'pg_params': {'root': './data/',\n",
    "                                            'transform': None}}}\n",
    "\n",
    "metrics_params = {'report_interval': 10,\n",
    "                  'log_plot': True,\n",
    "                  'sk_metric_name': 'roc_auc_score',\n",
    "                  'sk_params': {'average': 'macro',\n",
    "                                'multi_class': 'ovr'}}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "crit_params = {'adversarial': False,\n",
    "               'disc_params': {'in_channels': 32, 'hidden': 64, \n",
    "                                'out_channels': 32, 'softmax': None}}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7, .15),\n",
    "                 'subset': False}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "\n",
    "l = Learn([PGDS], GraphNetVariationalEncoder, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=EncoderLoss, DataLoader=DataLoader,\n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          metrics_params=metrics_params,\n",
    "          squeeze_y_pred=False, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example graph variational autoencoder with adversarial regulation and GCNConv encoder\n",
    "\n",
    "model_params = {'in_channels': 11, \n",
    "                'hidden': 32, \n",
    "                'out_channels': 32,\n",
    "                'depth': 2,\n",
    "                'softmax': None,\n",
    "                'pool': None,\n",
    "                'convolution': 'GCNConv'}\n",
    "\n",
    "ds_params = {'train_params': {'dataset': 'QM9',\n",
    "                              'transforms': {},\n",
    "                              'pg_params': {'root': './data/',\n",
    "                                            'transform': None}}}\n",
    "\n",
    "metrics_params = {'report_interval': 10,\n",
    "                  'log_plot': True,\n",
    "                  'sk_metric_name': 'roc_auc_score',\n",
    "                  'sk_params': {'average': 'macro',\n",
    "                                'multi_class': 'ovr'}}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "crit_params = {'adversarial': True,\n",
    "               'disc_params': {'in_channels': 32, 'hidden': 64, \n",
    "                                'out_channels': 32, 'softmax': None}}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7, .15),\n",
    "                 'subset': False}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "\n",
    "l = Learn([PGDS], GraphNetVariationalEncoder, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=EncoderLoss, DataLoader=DataLoader,\n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          metrics_params=metrics_params,\n",
    "          squeeze_y_pred=False, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example cosmosis/qchem PyG dataset wrapper with Data object output with\n",
    "#custom transforms and sklearn metrics and custom PyG model\n",
    "\n",
    "class IndexY(T.BaseTransform):\n",
    "    def __init__(self, i):\n",
    "        self.i = i\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        # Shallow-copy the data so that we prevent in-place data modification.\n",
    "        return self.forward(copy.copy(data))\n",
    "    \n",
    "    def forward(self, data):\n",
    "        y = data.y[:,self.i]\n",
    "        data.y = y\n",
    "        return data\n",
    "\n",
    "model_params = {'in_channels': 11, \n",
    "                'hidden': 256, \n",
    "                'out_channels': 1,\n",
    "                'depth': 2,\n",
    "                'convolution': 'SAGEConv',\n",
    "                'pool': 'MeanAggregation',\n",
    "                'dropout': .1,\n",
    "                'softmax': None,\n",
    "                'activation': 'relu',\n",
    "               }\n",
    "\n",
    "ds_params = {'train_params': {'dataset': 'QM9',\n",
    "                              'transforms': {},\n",
    "                              'pg_params': {'root': './data/',\n",
    "                                            'transform': T.Compose([IndexY(0), \n",
    "                                                                   ])}}}\n",
    "\n",
    "metrics_params = {'report_interval': 10,\n",
    "                  'log_plot': False}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7, .15)}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "\n",
    "l = Learn([PGDS], GraphNet, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=MSELoss, DataLoader=DataLoader,\n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          metrics_params=metrics_params,\n",
    "          squeeze_y_pred=True, batch_size=256, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example cosmosis/qchem PyG dataset wrapper with Data object output with\n",
    "#custom transforms and sklearn metrics and PyG model\n",
    "\n",
    "class IndexY(T.BaseTransform):\n",
    "    def __init__(self, i):\n",
    "        self.i = i\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        # Shallow-copy the data so that we prevent in-place data modification.\n",
    "        return self.forward(copy.copy(data))\n",
    "    \n",
    "    def forward(self, data):\n",
    "        y = data.y[:,self.i]\n",
    "        data.y = y\n",
    "        return data\n",
    "\n",
    "model_params = {'model_name': 'GraphSAGE',\n",
    "                'softmax': None,\n",
    "                'pool': 'MeanAggregation',\n",
    "                'ffnet': True,\n",
    "                'in_channels': 256, #ffnet params\n",
    "                'hidden': 256, \n",
    "                'out_channels': 1, \n",
    "                'pyg_params': {'in_channels': 11, #GraphSAGE params\n",
    "                               'hidden_channels': 256,\n",
    "                               'num_layers': 2,\n",
    "                               'out_channels': 256,\n",
    "                               'dropout': .1,\n",
    "                               'norm': 'BatchNorm'}}\n",
    "\n",
    "ds_params = {'train_params': {'dataset': 'QM9',\n",
    "                              'transforms': {},\n",
    "                              'pg_params': {'root': './data/',\n",
    "                                            'transform': T.Compose([IndexY(0)])}}}\n",
    "\n",
    "metrics_params = {'report_interval': 10,\n",
    "                  'log_plot': False}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7, .15)}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "\n",
    "l = Learn([PGDS], PygModel, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=MSELoss, DataLoader=DataLoader,\n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          metrics_params=metrics_params,\n",
    "          squeeze_y_pred=True, batch_size=256, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example cosmosis/qchem PyG dataset wrapper with custum transform with\n",
    "#dictionary output and custom cosmosis model\n",
    "\n",
    "model_params = {'in_channels': 319, \n",
    "                'hidden': 512, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel'}\n",
    "\n",
    "ds_params = {'train_params': {'dataset': 'QM9',\n",
    "                              'transforms': {'y': [Index(0)],\n",
    "                                             'x': [Pad((29,11)), Flatten()]},\n",
    "                              'input_dict': {'model_input': {'X': ['x']},\n",
    "                                             'criterion_input': {'target': ['y']}},\n",
    "                              'pg_params': {'root': './data/'}}}\n",
    "\n",
    "metrics_params = {'report_interval': 10,\n",
    "                  'log_plot': False}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7, .15)}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "\n",
    "l = Learn([PGDS], FFNet, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=MSELoss, \n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          metrics_params=metrics_params,\n",
    "          batch_size=256, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example using cosmosis/qchem QM7 dataset and custom cosmosis model\n",
    "model_params = {'in_channels': 23*23, \n",
    "                'hidden': 2048, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel'}\n",
    "\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': ['coulomb']},\n",
    "                                             'criterion_input': {'target': ['ae']}},\n",
    "                              'in_file': './data/qm7/qm7.mat',\n",
    "                              'transforms': {'coulomb': [Flatten()]}}}\n",
    "\n",
    "metrics_params = {'report_interval': 10,\n",
    "                  'log_plot': True}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15)}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "l = Learn([QM7], FFNet, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=L1Loss, \n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          metrics_params=metrics_params,\n",
    "          batch_size=256, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example using the cosmosis/qchem QM7b dataset and custom cosmosis model\n",
    "model_params = {'in_channels': 23*23, \n",
    "                'hidden': 2048, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel'}\n",
    "\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': ['coulomb']},\n",
    "                                             'criterion_input': {'target': ['E']}},\n",
    "                              'in_file': './data/qm7b/qm7b.mat',\n",
    "                              'transforms': {'coulomb': [Flatten()]}}}\n",
    "\n",
    "metrics_params = {'report_interval': 10,\n",
    "                  'log_plot': True}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15)}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "l = Learn([QM7b], FFNet, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=L1Loss, \n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          metrics_params=metrics_params, \n",
    "          batch_size=256, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': ['coulomb']},\n",
    "                                             'criterion_input': {'target': ['E']}},\n",
    "                              'in_file': './data/qm7b/qm7b.mat',\n",
    "                              'transforms': {'coulomb': [Flatten()]}}}\n",
    "\n",
    "qm7b = QM7b(**ds_params['train_params'])\n",
    "qm7b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm7b[1]['model_input']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example exploring cosmosis/qchem QM7X dataset\n",
    "\n",
    "features = ['DIP','HLgap','atC6','atNUM','atPOL','atXYZ','eAT','hDIP',\n",
    "            'eC','eDFTB+MBD','eEE','eH','eKIN','eKSE','eL','eMBD','eNE', \n",
    "            'eNN','ePBE0','ePBE0+MBD','eTS','eX','eXC','eXX','hCHG', \n",
    "            'hRAT','hVDIP','hVOL','mC6','mPOL','mTPOL','pbe0FOR', \n",
    "            'sMIT','sRMSD','totFOR','vDIP','vEQ','vIQ','vTQ','vdwFOR','vdwR',\n",
    "            'coulomb'] #(1110,) with pad=23\n",
    "\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': ['atNUM']},\n",
    "                                             'criterion_input': {'target': ['eAT']}},\n",
    "                              'transforms': {},\n",
    "                              'n': 1000, #non-random subset for testing\n",
    "                              'selector': ['opt']}}\n",
    "\n",
    "qm7x = QM7X(**ds_params['train_params'])\n",
    "\n",
    "l, m = 0, 0\n",
    "for i in qm7x.ds_idx:\n",
    "    s = qm7x[i]['model_input']['X'].shape[0]\n",
    "    if s > l:\n",
    "        l = s\n",
    "        m = i\n",
    "print('longest molecule length: ', l, ' index: ', m)\n",
    "print(qm7x[m]['model_input']['X'].shape) \n",
    "qm7x[m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example using the cosmosis/qchem QM7X dataset with filter and custom cosmosis model\n",
    "model_params = {'in_channels': 23*23,\n",
    "                'hidden': 2048, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel'}\n",
    "\n",
    "features = ['DIP','HLgap','atC6','atNUM','atPOL','atXYZ','hDIP','eAT',\n",
    "            'eC','eDFTB+MBD','eEE','eH','eKIN','eKSE','eL','eMBD','eNE', \n",
    "            'eNN','ePBE0','ePBE0+MBD','eTS','eX','eXC','eXX','hCHG', \n",
    "            'hRAT','hVDIP','hVOL','mC6','mPOL','mTPOL','pbe0FOR', \n",
    "            'sMIT','sRMSD','totFOR','vDIP','vEQ','vIQ','vTQ','vdwFOR','vdwR',\n",
    "            'coulomb','distance']\n",
    "\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': ['coulomb']},\n",
    "                                             'criterion_input': {'target': ['eAT']}},\n",
    "                              'transforms': {'coulomb': [Pad((23,23)), Flatten()]},\n",
    "                              'selector': ['opt'],\n",
    "                              'n': 1000, #non-random subset for testing\n",
    "                             }}\n",
    "\n",
    "metrics_params = {'report_interval': 10,\n",
    "                  'log_plot': True}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15),\n",
    "                 #'subset': .2 #create random subset\n",
    "                } \n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "l = Learn([QM7X], FFNet, Selector, \n",
    "          Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=L1Loss, \n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          metrics_params=metrics_params,\n",
    "          batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example using cosmosis/qchem ANI1x dataset with filter and custom cosmosis model\n",
    "\n",
    "model_params = {'in_channels': 63*63, #length of the longest molecule in the dataset\n",
    "                'hidden': 2000, \n",
    "                'out_channels': 1, \n",
    "                'model_name': 'funnel'\n",
    "               }\n",
    "\n",
    "ds_params = {'train_params': {'input_dict': {'model_input': {'X': ['coulomb']},\n",
    "                                             'criterion_input': {'target': ['wb97x_dz.energy']}},\n",
    "                              'transforms': {'coulomb': [Pad((63,63)), Flatten()]},\n",
    "                              'criterion': ['wb97x_dz.energy'],\n",
    "                              'conformation': 'max',\n",
    "                              'in_file': './data/ani1x/ani1x-release.h5'}}\n",
    "\n",
    "metrics_params = {'report_interval': 20}\n",
    "\n",
    "crit_params = {'reduction': 'sum'}\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15)}\n",
    "\n",
    "sched_params = {'factor': .5,\n",
    "                'patience': 2,\n",
    "                'cooldown': 1,\n",
    "                'min_lr': .005}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "\n",
    "l = Learn([ANI1x], FFNet, Selector, Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=L1Loss, \n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
